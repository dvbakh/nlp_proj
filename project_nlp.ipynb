{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a750e785",
   "metadata": {},
   "source": [
    "# Проект по Автобрее \"Корпус текстов Братьев Стругацких\"\n",
    "## Часть 1 Создание корпуса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ef9c03",
   "metadata": {},
   "source": [
    "#### Выполнен студентом группы БКЛ211 Бахваловым Дмитрием"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ae41c",
   "metadata": {},
   "source": [
    "### Ход работы\n",
    "\n",
    "Сначала подготовлю корпус текстов для проекта. Идея состоит в том, чтобы сделать корпус текстов Аркадия и Бориса Стругацких, но так как есть ограничение в 10000 слов и необходимо не менее 100 текстов, то мы пробежимся по их произведениям и из каждого возьмем немного текста из начала произведения. Делали это мы с помощью краулера, а обкачивали сайт strugacki.ru. Ниже идет код краулера и создание с его помощью базы данных. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a001c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fake-useragent in c:\\users\\dimba\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install fake-useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56395e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fake_useragent import UserAgent\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "\n",
    "session = requests.session()\n",
    "page_number=1\n",
    "url = f'https://strugacki.ru/book_1/{page_number}.html'\n",
    "req = session.get(url, headers={'User-Agent': ua.random})\n",
    "page = req.text\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "book_pages = soup.find('div', {'class': 'cont'}).find('p', {'class': 'text'}).find_all('a')\n",
    "next_book_page=len(book_pages)+2\n",
    "next_book_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bfa3f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Эта функция позволяет узнать, на какой странице начинается новая книга, т.к. нумерация страниц сплошная\n",
    "def new_book_page(page_number, counter):\n",
    "    url = f'https://strugacki.ru/book_{counter}/{page_number}.html'\n",
    "    req = session.get(url, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    book_pages = soup.find('div', {'class': 'cont'}).find('p', {'class': 'text'}).find_all('a')\n",
    "    next_book_page=len(book_pages)+int(page_number)+1\n",
    "    old_book_title = soup.find('div', {'class': 'cont'}).find('h1').text.split('>')[1].strip()\n",
    "    return next_book_page, old_book_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f6ac621f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Трудно быть богом',\n",
       " 60: 'Пикник на обочине',\n",
       " 98: 'Парень из преисподней',\n",
       " 122: 'За миллиард лет до конца света',\n",
       " 154: 'Беспокойство (Улитка на склоне-1)',\n",
       " 180: 'Путь на Амальтею',\n",
       " 200: 'Малыш',\n",
       " 238: 'Дело об убийстве, или отель \"У погибшего альпиниста\"',\n",
       " 286: 'Хищные вещи века',\n",
       " 331: 'Стажеры',\n",
       " 390: 'Полдень, XXII век (Возвращение)',\n",
       " 455: 'Далекая Радуга'}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di={}\n",
    "di[1]='-'\n",
    "ar=['1']\n",
    "for i in ar:\n",
    "    try:\n",
    "        di.update({new_book_page(i, len(di))[0]:new_book_page(i, len(di))[1]})\n",
    "        ar.append(new_book_page(i, len(di))[0])\n",
    "    except:\n",
    "        Exception\n",
    "#Изначально мне нужно было знать не только где начинается, но и где заканчивается текст произведения. \n",
    "#В конечном словаре это лишнее, поэтому сдвигаю значения и убираю последний ключ.\n",
    "keys=list(di.keys())\n",
    "val=list(di.values())\n",
    "di=dict(zip(keys, val[1:len(val)+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c9f8f",
   "metadata": {},
   "source": [
    "Процесс пошел, но только до страницы 487 (это я узнал из прошлой ячейки до сдвига значений словаря). Я посмотрел сайт, в чем же проблема, и там дальше нет страниц до 544. Вероятно, была какая-то книга, которую в итоге удалили. Ладно, берем теперь за точку отсчета 544 и едем дальше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "46a370c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Трудно быть богом',\n",
       " 60: 'Пикник на обочине',\n",
       " 98: 'Парень из преисподней',\n",
       " 122: 'За миллиард лет до конца света',\n",
       " 154: 'Беспокойство (Улитка на склоне-1)',\n",
       " 180: 'Путь на Амальтею',\n",
       " 200: 'Малыш',\n",
       " 238: 'Дело об убийстве, или отель \"У погибшего альпиниста\"',\n",
       " 286: 'Хищные вещи века',\n",
       " 331: 'Стажеры',\n",
       " 390: 'Полдень, XXII век (Возвращение)',\n",
       " 455: 'Далекая Радуга',\n",
       " 544: 'Дьявол среди людей',\n",
       " 577: 'Дни затмения',\n",
       " 593: 'Обитаемый остров',\n",
       " 673: 'Попытка к бегству',\n",
       " 703: 'Жук в муравейнике',\n",
       " 758: 'Улитка на склоне',\n",
       " 807: 'Подробности жизни Никиты Воронцова',\n",
       " 817: 'Экспедиция в преисподнюю',\n",
       " 879: 'Второе нашествие марсиан',\n",
       " 903: 'Гадкие  лебеди',\n",
       " 954: 'Волны гасят ветер',\n",
       " 1003: 'Град  обреченный',\n",
       " 1099: 'Понедельник начинается в субботу',\n",
       " 1155: 'Сказка о тройке',\n",
       " 1200: 'Отягощенные злом, или сорок лет спустя',\n",
       " 1252: 'Страна багровых туч',\n",
       " 1328: 'Без оружия',\n",
       " 1349: 'Благоустроенная планета',\n",
       " 1356: 'Хромая судьба',\n",
       " 1438: 'Четвертое царство (На грани возможного)',\n",
       " 1453: 'Дни Кракена',\n",
       " 1472: 'Извне',\n",
       " 1486: 'Бессильные мира сего',\n",
       " 1551: 'Повесть о дружбе и недружбе',\n",
       " 1563: 'Жиды города Питера, или невеселые беседы при свечах',\n",
       " 1574: 'Дело об убийстве (Отель \"У погибшего альпиниста\")',\n",
       " 1588: 'День затмения',\n",
       " 1603: 'Машина желаний',\n",
       " 1614: 'Пять ложек эликсира',\n",
       " 1630: 'Сталкер',\n",
       " 1640: 'Туча',\n",
       " 1652: 'Чародеи',\n",
       " 1665: 'Бедные злые люди',\n",
       " 1666: 'Белый конус Алаида',\n",
       " 1671: 'В наше интересное время',\n",
       " 1672: 'Забытый эксперимент',\n",
       " 1677: 'Испытание \"СКИБР\"',\n",
       " 1682: 'Моби Дик',\n",
       " 1686: 'Ночь на Марсе',\n",
       " 1689: 'О странствующих и путешествующих',\n",
       " 1691: 'Первые люди на первом плоту',\n",
       " 1693: 'Песчаная горячка',\n",
       " 1695: 'Спонтанный рефлекс',\n",
       " 1699: 'Человек из Пасифиды',\n",
       " 1704: 'Чрезвычайное происшествие',\n",
       " 1707: 'Частные предположения',\n",
       " 1713: 'Шесть спичек'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "di_1={}\n",
    "di_1[544]='-'\n",
    "ar_1=[544]\n",
    "for i in ar_1:\n",
    "    try:\n",
    "        di_1.update({new_book_page(i, len(di_1)+12)[0]:new_book_page(i, len(di_1)+12)[1]})\n",
    "        ar_1.append(new_book_page(i, len(di_1)+12)[0])\n",
    "    except:\n",
    "        Exception\n",
    "keys_1=list(di_1.keys())\n",
    "val_1=list(di_1.values())\n",
    "di_1=dict(zip(keys_1, val_1[1:len(val_1)+1]))\n",
    "#Склеим теперь словари в один\n",
    "di.update(di_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65238873",
   "metadata": {},
   "source": [
    "Выкачались все произведенния с этого сайта. К сожалению, получилось их меньше 100, всего примерно 70 штук. В таком случае, возьмем из них чуть больше, чем по 100 слов из каждого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7649525c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in di.keys():\n",
    "    url = f'https://strugacki.ru/book_1/{i}.html'\n",
    "    req = session.get(url, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    full = soup.find('div', {'class': 'cont'}).find('p', {'align':'justify'}).text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35341a7",
   "metadata": {},
   "source": [
    "А теперь загрузим все в базу данных, параллельно добавляя тексты первых страниц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ef842c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import unicodedata\n",
    "\n",
    "conn = sqlite3.connect('my_corpus.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS db_corp \n",
    "(id INTEGER PRIMARY KEY AUTOINCREMENT, page_number int, title text, full_text text)\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "for i in di.keys():\n",
    "    url = f'https://strugacki.ru/book_1/{i}.html'\n",
    "    req = session.get(url, headers={'User-Agent': ua.random})\n",
    "    page = req.text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    full = soup.find('div', {'class': 'cont'}).find_all('p', {'class':'text', 'align':'justify'})\n",
    "    for p in full:\n",
    "        d = unicodedata.normalize('NFKD', p.text).strip()\n",
    "    if d != '':\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            INSERT INTO db_corp \n",
    "                (page_number, title, full_text)\n",
    "                VALUES (?, ?, ?)\n",
    "            \"\"\", (\n",
    "                i,\n",
    "                di[i],\n",
    "                d)\n",
    "        )\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c373992",
   "metadata": {},
   "source": [
    "Отлично, база данных готова. Наверное, для удобства было бы отлично сделать еще одну таблицу с предложениями из каждого текста.\n",
    "Наверное, для оптимизации будет хорошо, если эти предложения еще сразу разметить. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3070c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 15:11:39 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eef83fcf78b4200bdb41faaaa1bb267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-23 15:11:40 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| pos       | syntagrus_charlm   |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "==================================\n",
      "\n",
      "2023-10-23 15:11:40 INFO: Using device: cpu\n",
      "2023-10-23 15:11:40 INFO: Loading: tokenize\n",
      "2023-10-23 15:11:40 INFO: Loading: pos\n",
      "2023-10-23 15:11:41 INFO: Loading: lemma\n",
      "2023-10-23 15:11:41 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import unicodedata\n",
    "import stanza\n",
    "\n",
    "stz = stanza.Pipeline(lang='ru', processors='tokenize,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7ca6cae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('my_corpus (2).db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS db_meta\n",
    "(page_number INT, sentence TEXT, words TEXT)\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "data = \"\"\"SELECT page_number, full_text FROM db_corp\"\"\"\n",
    "cur.execute(data)\n",
    "records = cur.fetchall()\n",
    "for i in records:\n",
    "    doc=stz(i[1])\n",
    "    for a, snt in enumerate(doc.sentences):\n",
    "        #думаю, для проекта по 25  предложений из каждой книги будет достаточно, так что на этом этапе отсеку все остальное\n",
    "        if a<25:\n",
    "            f=''\n",
    "            for word in snt.words:\n",
    "                meta='='.join([word.text, word.upos, word.lemma]) + ' '\n",
    "                f+=meta\n",
    "            cur.execute(\n",
    "                \"\"\"\n",
    "                INSERT INTO db_meta\n",
    "                    (page_number, sentence, words)\n",
    "                    VALUES (?, ?, ?)\n",
    "                \"\"\", (\n",
    "                        i[0],\n",
    "                        snt.text,\n",
    "                        f)\n",
    "            )\n",
    "            conn.commit()\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974b6e3",
   "metadata": {},
   "source": [
    "## Часть 2. Создание функций поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9367eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 01:01:40 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33916bf98d3455d80595c58be3f9a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-26 01:01:42 INFO: Loading these models for language: ru (Russian):\n",
      "==================================\n",
      "| Processor | Package            |\n",
      "----------------------------------\n",
      "| tokenize  | syntagrus          |\n",
      "| pos       | syntagrus_charlm   |\n",
      "| lemma     | syntagrus_nocharlm |\n",
      "==================================\n",
      "\n",
      "2023-10-26 01:01:42 INFO: Using device: cpu\n",
      "2023-10-26 01:01:42 INFO: Loading: tokenize\n",
      "2023-10-26 01:01:42 INFO: Loading: pos\n",
      "2023-10-26 01:01:44 INFO: Loading: lemma\n",
      "2023-10-26 01:01:44 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import unicodedata\n",
    "import stanza\n",
    "\n",
    "stz = stanza.Pipeline(lang='ru', processors='tokenize,pos,lemma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2e6cc6",
   "metadata": {},
   "source": [
    "Близясь к концу кода я понял, что удобнее и быстрее всего выполнить поиск только один раз - по первому слову в инпуте. Дальше можно запомнить два слова, которые после, и остальные фукнции проверять уже на них. Из-за этого данный блок я перекраивал несколько раз, так что комментарии немного перепутались, но я постарался восстановить все как было"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b33a0648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_word(some_word):\n",
    "    conn = sqlite3.connect('my_corpus (2).db')\n",
    "    cur = conn.cursor()\n",
    "    data = \"\"\"\n",
    "    SELECT db_meta.page_number, sentence, words, title FROM db_meta\n",
    "    JOIN db_corp ON db_meta.page_number = db_corp.page_number\n",
    "    \"\"\"\n",
    "    seen=[]\n",
    "    cur.execute(data)\n",
    "    textts=cur.fetchall()\n",
    "    for row in textts:\n",
    "        lst_rows=row[2].split()\n",
    "        for n_l in range(len(lst_rows)):\n",
    "            wrd=lst_rows[n_l].split('=')\n",
    "            if wrd[0] == some_word[1:-1]:\n",
    "                if n_l+2 < len(lst_rows):\n",
    "                    seen.append([row[1], row[3], lst_rows[n_l+1], lst_rows[n_l+2]])\n",
    "                elif n_l+1 < len(lst_rows):\n",
    "                    seen.append([row[1], row[3], lst_rows[n_l+1], 'null'])\n",
    "                else:\n",
    "                    seen.append([row[1], row[3], 'null', 'null'])\n",
    "    conn.close()\n",
    "    return(seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "928d648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_word(some_word):\n",
    "    conn = sqlite3.connect('my_corpus (2).db')\n",
    "    cur = conn.cursor()\n",
    "    data = \"\"\"\n",
    "    SELECT db_meta.page_number, sentence, words, title FROM db_meta\n",
    "    JOIN db_corp ON db_meta.page_number = db_corp.page_number\n",
    "    \"\"\"\n",
    "    seen=[]\n",
    "    cur.execute(data)\n",
    "    textts=cur.fetchall()\n",
    "    doc=stz(some_word)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            some_word_lem=word.lemma\n",
    "    for row in textts:\n",
    "        k=0\n",
    "        lst_rows=row[2].split()\n",
    "        for n_l in range(len(lst_rows)):\n",
    "            k+=1\n",
    "            wrd=lst_rows[n_l].split('=')\n",
    "            if wrd[2] == some_word_lem:\n",
    "                if n_l+2 < len(lst_rows):\n",
    "                    seen.append([row[1], row[3], lst_rows[n_l+1], lst_rows[n_l+2]])\n",
    "                elif n_l+1 < len(lst_rows):\n",
    "                    seen.append([row[1], row[3], lst_rows[n_l+1], 'null'])\n",
    "                else:\n",
    "                    seen.append([row[1], row[3], 'null', 'null'])\n",
    "    conn.close()\n",
    "    return(seen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5131e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_and_pos(some_word):\n",
    "    some_word=some_word.split('+')\n",
    "    conn = sqlite3.connect('my_corpus (2).db')\n",
    "    cur = conn.cursor()\n",
    "    data = \"\"\"\n",
    "    SELECT db_meta.page_number, sentence, words, title FROM db_meta\n",
    "    JOIN db_corp ON db_meta.page_number = db_corp.page_number\n",
    "    \"\"\"\n",
    "    seen=[]\n",
    "    cur.execute(data)\n",
    "    textts=cur.fetchall()\n",
    "    for row in textts:\n",
    "        lst_rows=row[2].split()\n",
    "        for n_l in range(len(lst_rows)):\n",
    "            wrd=lst_rows[n_l].split('=')\n",
    "            if wrd[0] == some_word[0] and wrd[1] == some_word[1]:\n",
    "                if n_l+2 < len(lst_rows):\n",
    "                    seen.append([row[1], row[3], lst_rows[n_l+1], lst_rows[n_l+2]])\n",
    "                elif n_l+1 < len(lst_rows):\n",
    "                    seen.append([row[1], row[3], lst_rows[n_l+1], 'null'])\n",
    "                else:\n",
    "                    seen.append([row[1], row[3], 'null', 'null'])\n",
    "    conn.close()\n",
    "    return(seen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabac7cc",
   "metadata": {},
   "source": [
    "Вид запроса слово+POS у нас ищет слова именно в этой форме.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db186287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_word(pos):\n",
    "    conn = sqlite3.connect('my_corpus (2).db')\n",
    "    cur = conn.cursor()\n",
    "    data = \"\"\"\n",
    "    SELECT db_meta.page_number, sentence, words, title FROM db_meta\n",
    "    JOIN db_corp ON db_meta.page_number = db_corp.page_number\n",
    "    \"\"\"\n",
    "    seen=[]\n",
    "    cur.execute(data)\n",
    "    textts=cur.fetchall()\n",
    "    for row in textts:\n",
    "        lst_rows=row[2].split()\n",
    "        for n_l in range(len(lst_rows)):\n",
    "            wrd=lst_rows[n_l].split('=')\n",
    "            if wrd[1] == pos:\n",
    "                if n_l+2 < len(lst_rows):\n",
    "                    seen.append([row[1], row[3], lst_rows[n_l+1], lst_rows[n_l+2]])\n",
    "                elif n_l+1 < len(lst_rows):\n",
    "                    seen.append([row[1], row[3], lst_rows[n_l+1], 'null'])\n",
    "                else:\n",
    "                    seen.append([row[1], row[3], 'null', 'null'])\n",
    "    conn.close()\n",
    "    return(seen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aac757",
   "metadata": {},
   "source": [
    "Вот функция для записи одного слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb0dba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#на вход str\n",
    "def for_one_word(word_s):\n",
    "    frst_wrd=[]\n",
    "    if word_s[0]=='\"':\n",
    "        for i in exact_word(word_s):\n",
    "            frst_wrd.append([i[0], i[1], i[2], i[3]])\n",
    "    elif word_s[0] in 'QWERTYUIOPASDFGHJKLZXCVBNM':\n",
    "        for i in pos_word(word_s):\n",
    "            frst_wrd.append([i[0], i[1], i[2], i[3]])\n",
    "    elif len(word_s.split('+'))==2:\n",
    "        for i in word_and_pos(word_s):\n",
    "            frst_wrd.append([i[0], i[1], i[2], i[3]])\n",
    "    else:\n",
    "        for i in lemma_word(word_s):\n",
    "            frst_wrd.append([i[0], i[1], i[2], i[3]])\n",
    "    return(frst_wrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "140fe65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#на вход получаем список из двух слов, по которым поиск\n",
    "def for_two_words(word_s):\n",
    "    two_wrds=[]\n",
    "    for o in for_one_word(word_s[0]):\n",
    "        if word_s[1][0]=='\"':\n",
    "            if o[2].split('=')[0] == word_s[1][1:-1]:\n",
    "                two_wrds.append(o)\n",
    "        elif word_s[1][0] in 'QWERTYUIOPASDFGHJKLZXCVBNM':\n",
    "            if o[2].split('=')[1] == word_s[1]:\n",
    "                two_wrds.append(o)\n",
    "        elif len(word_s[1].split('+'))==2:\n",
    "            if o[2].split('=')[1] == word_s[1].split('+')[1] and o[2].split('=')[0] == word_s[1].split('+')[0]:\n",
    "                two_wrds.append(o)\n",
    "        else:\n",
    "            doc=stz(word_s[1])\n",
    "            for sent in doc.sentences:\n",
    "                for word in sent.words:\n",
    "                    word_lem=word.lemma\n",
    "            if o[2].split('=')[2] == word_lem:\n",
    "                two_wrds.append(o)\n",
    "    return(two_wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66b89bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_three_words(word_s):\n",
    "    three_wrds=[]\n",
    "    for o in for_two_words(word_s[:2]):\n",
    "        if word_s[2][0]=='\"':\n",
    "            if o[3].split('=')[0] == word_s[2][1:-1]:\n",
    "                three_wrds.append(o)\n",
    "        elif word_s[2][0] in 'QWERTYUIOPASDFGHJKLZXCVBNM':\n",
    "            if o[3].split('=')[1] == word_s[2]:\n",
    "                three_wrds.append(o)\n",
    "        elif len(word_s[2].split('+'))==2:\n",
    "            if o[3].split('=')[1] == word_s[2].split('+')[1] and o[3].split('=')[0] == word_s[2].split('+')[0]:\n",
    "                three_wrds.append(o)\n",
    "        else:\n",
    "            doc=stz(word_s[2])\n",
    "            for sent in doc.sentences:\n",
    "                for word in sent.words:\n",
    "                    word_lem=word.lemma\n",
    "            if o[3].split('=')[2] == word_lem:\n",
    "                three_wrds.append(o)\n",
    "    return(three_wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e2e27d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(inp):\n",
    "    d=[]\n",
    "    if len(inp) == 1:\n",
    "        for o in for_one_word(inp[0]):\n",
    "            d.append(o)\n",
    "    elif len(inp) == 2:\n",
    "        for o in for_two_words(inp[:2]):\n",
    "            d.append(o)\n",
    "    elif len(inp) == 3:\n",
    "        for o in for_three_words(inp[:3]):\n",
    "            d.append(o)\n",
    "    return d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
